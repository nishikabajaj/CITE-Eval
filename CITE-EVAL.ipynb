{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing & feature engineering"
      ],
      "metadata": {
        "id": "1WaEGiVXLKIE"
      },
      "id": "1WaEGiVXLKIE"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import xgboost as xgb\n",
        "import joblib, os\n",
        "import matplotlib.pyplot as plt\n",
        "import shap\n",
        "from tqdm import tqdm\n",
        "print('Notebook environment ready.')\n",
        "\n",
        "SEED = 42"
      ],
      "metadata": {
        "id": "rY8KMknnLQja",
        "outputId": "4d5f259b-fd95-4a05-85ac-6624316a3a0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "rY8KMknnLQja",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Notebook environment ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Dcmp4mrIQ3Sr",
        "outputId": "08b4f284-604d-44cc-ebfb-89a171891a77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Dcmp4mrIQ3Sr",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------\n",
        "# Helper function: compute Δ-FPR fairness metric\n",
        "# ---------------------------------------------\n",
        "def compute_delta_fpr(y_true, y_pred, clusters):\n",
        "    results = []\n",
        "    for k in np.unique(clusters):\n",
        "        idx = (clusters == k)\n",
        "        if idx.sum() == 0:\n",
        "            continue\n",
        "        true = y_true[idx]\n",
        "        pred = y_pred[idx]\n",
        "        tn = ((true==0)&(pred==0)).sum()\n",
        "        fp = ((true==0)&(pred==1)).sum()\n",
        "        fpr = fp / (fp + tn + 1e-9)\n",
        "        results.append(fpr)\n",
        "    if len(results) == 0:\n",
        "        return np.nan\n",
        "    global_fpr = np.mean(results)\n",
        "    return max(abs(f - global_fpr) for f in results)\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Helper function: evaluate any anomaly score\n",
        "# ---------------------------------------------\n",
        "def eval_model(y_true, scores):\n",
        "    auc = roc_auc_score(y_true, scores)\n",
        "    thresh = np.percentile(scores, 99)  # anomaly threshold\n",
        "    pred = (scores > thresh).astype(int)\n",
        "    _, _, f1, _ = precision_recall_fscore_support(\n",
        "        y_true, pred, average='binary', zero_division=0\n",
        "    )\n",
        "    return auc, f1, pred"
      ],
      "metadata": {
        "id": "d4XLYKYlxzpP"
      },
      "id": "d4XLYKYlxzpP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunk loader function\n",
        "# path - file path to dataset (CERT 6.2) CSV log file\n",
        "# usecols - file-specific features to use for evaluation\n",
        "\n",
        "def load_chunked(path, usecols, chunksize=200_000):\n",
        "  iter = pd.read_csv(path, compression='gzip', chunksize=chunksize, usecols=usecols)\n",
        "  return pd.concat([chunk for chunk in iter])"
      ],
      "metadata": {
        "id": "TNlthNHYMZ74"
      },
      "id": "TNlthNHYMZ74",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load LOGON data\n",
        "logon_cols = ['user', 'pc', 'time', 'activity']\n",
        "logon = load_chunked('/content/drive/MyDrive/BS-Nishika/cert6.2/logon.csv.gz', logon_cols)\n",
        "\n",
        "logon['time'] = pd.to_datetime(logon['time'])\n",
        "logon['day'] = logon['time'].dt.date\n",
        "logon.head()"
      ],
      "metadata": {
        "id": "_ofVBud_MdOp"
      },
      "id": "_ofVBud_MdOp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract LOGON daily features\n",
        "logon_daily = logon.groupby(['user','day']).agg(\n",
        "    logins=('activity','count'),\n",
        "    unique_hosts=('pc','nunique'),\n",
        "    avg_login_hour=('time', lambda x: x.dt.hour.mean())\n",
        ").reset_index()"
      ],
      "metadata": {
        "id": "CgwTHydbkAJ8"
      },
      "id": "CgwTHydbkAJ8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load FILE data and extract daily features\n",
        "file_cols = ['user', 'path', 'time']\n",
        "file = load_chunked('/content/drive/MyDrive/BS-Nishika/cert6.2/file.csv.gz', file_cols)\n",
        "\n",
        "file['time'] = pd.to_datetime(file['time'])\n",
        "file['day'] = file['time'].dt.date\n",
        "\n",
        "file_daily = file.groupby(['user','day']).agg(\n",
        "    files_accessed=('path','nunique')\n",
        ").reset_index()"
      ],
      "metadata": {
        "id": "Qyw9l8eYkH5D"
      },
      "id": "Qyw9l8eYkH5D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load DEVICE data and extract daily features\n",
        "device_cols = ['user','time','activity']\n",
        "device = load_chunked('/content/drive/MyDrive/cert6.2/device.csv.gz', device_cols)\n",
        "\n",
        "device['time'] = pd.to_datetime(device['time'])\n",
        "device['day'] = device['time'].dt.date\n",
        "\n",
        "device_daily = device.groupby(['user','day']).agg(\n",
        "    device_events=('activity','count')\n",
        ").reset_index()"
      ],
      "metadata": {
        "id": "7iNTLNo8kOIj"
      },
      "id": "7iNTLNo8kOIj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge all behavioral features\n",
        "features = logon_daily.merge(file_daily, on=['user','day'], how='left')\n",
        "features = features.merge(device_daily, on=['user','day'], how='left')\n",
        "\n",
        "features.fillna(0, inplace=True)\n",
        "features.head()"
      ],
      "metadata": {
        "id": "jS1gbv-7kY1Z"
      },
      "id": "jS1gbv-7kY1Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add cultural proxy features (safe, non PII - key being that they remain as behavioral patterns, not identity)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Proxy 1: Work shift bucket (0–3) based on avg login hour\n",
        "features['shift_bucket'] = (features['avg_login_hour'] // 6).astype(int)\n",
        "\n",
        "# Proxy 2: Communication density proxy (log of logins)\n",
        "features['comm_density'] = np.log1p(features['logins'])\n",
        "\n",
        "# Proxy 3: Synthetic team_id inferred from user hash (stable, non-sensitive)\n",
        "features['team_id'] = features['user'].astype('category').cat.codes % 6\n"
      ],
      "metadata": {
        "id": "qQGLCZzYkonW"
      },
      "id": "qQGLCZzYkonW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load LABELS and merge to features\n",
        "labels = pd.read_csv('/content/drive/MyDrive/BS-Nishika/cert6.2/insider-threat6.2_label.csv.gz',\n",
        "                     compression='gzip')\n",
        "\n",
        "# Simplify for evaluation: Treat any user with any malicious tag as malicious on all days\n",
        "mal_users = set(labels['user'][labels['label']==1])\n",
        "features['label'] = features['user'].apply(lambda u: 1 if u in mal_users else 0)\n"
      ],
      "metadata": {
        "id": "XIMaqVUzlPtj"
      },
      "id": "XIMaqVUzlPtj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FINAL ML FEATURE MATRIX\n",
        "feat_cols = [\n",
        "    'logins', 'unique_hosts', 'files_accessed', 'device_events',\n",
        "    'avg_login_hour', 'shift_bucket', 'comm_density', 'team_id'\n",
        "]\n",
        "\n",
        "X = features[feat_cols]\n",
        "y = features['label'].values\n",
        "\n",
        "# One-hot team encoding (except first)\n",
        "X = pd.get_dummies(X, columns=['team_id'], drop_first=True)\n",
        "\n",
        "scaler = StandardScaler().fit(X)\n",
        "Xs = scaler.transform(X)\n",
        "\n",
        "Xtr, Xte, ytr, yte = train_test_split(Xs, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "pT8BxFNNldY1"
      },
      "id": "pT8BxFNNldY1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating Baseline Model\n",
        "Global IsolationForest"
      ],
      "metadata": {
        "id": "iPm0QVALlvKg"
      },
      "id": "iPm0QVALlvKg"
    },
    {
      "cell_type": "code",
      "source": [
        "baseline = IsolationForest(n_estimators=200, contamination=0.01, random_state=42)\n",
        "baseline.fit(Xtr)\n",
        "scores = -baseline.decision_function(Xte)\n",
        "\n",
        "auc_base = roc_auc_score(yte, scores)\n",
        "pred_base = (scores > np.percentile(scores, 99)).astype(int)\n",
        "prec, rec, f1, _ = precision_recall_fscore_support(yte, pred_base, average='binary')\n",
        "\n",
        "print(\"Baseline IF — AUC:\", auc_base, \"F1:\", f1)"
      ],
      "metadata": {
        "id": "2Cc6Z179qaM3"
      },
      "id": "2Cc6Z179qaM3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing CITE Pipeline\n",
        "GMM Clustering + Per-Cluster IsolationForest (RL STREAM)"
      ],
      "metadata": {
        "id": "GV7K1JgLONP3"
      },
      "id": "GV7K1JgLONP3"
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit Gaussian Mixture Model (GMM)\n",
        "gmm = GaussianMixture(n_components=6, random_state=42)\n",
        "gmm.fit(Xtr)\n",
        "\n",
        "cluster_tr = gmm.predict(Xtr)\n",
        "cluster_te = gmm.predict(Xte)"
      ],
      "metadata": {
        "id": "zF7heuCVOS2s"
      },
      "id": "zF7heuCVOS2s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train per-cluster IF models\n",
        "cluster_models = {}\n",
        "cluster_thresh = {}\n",
        "\n",
        "for k in range(6):\n",
        "    idx = (cluster_tr == k)\n",
        "    if idx.sum() < 40:\n",
        "        continue\n",
        "    sub = Xtr[idx]\n",
        "\n",
        "    clf_k = IsolationForest(n_estimators=200, random_state=42).fit(sub)\n",
        "    scores_k = -clf_k.decision_function(sub)\n",
        "\n",
        "    cluster_models[k] = clf_k\n",
        "    cluster_thresh[k] = np.percentile(scores_k, 99)"
      ],
      "metadata": {
        "id": "mns8z8S1OVVG"
      },
      "id": "mns8z8S1OVVG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Infer cluster-model anomaly scores\n",
        "scores_cite = []\n",
        "\n",
        "for x, k in zip(Xte, cluster_te):\n",
        "    if k in cluster_models:\n",
        "        s = -cluster_models[k].decision_function(x.reshape(1,-1))\n",
        "        scores_cite.append(s[0])\n",
        "    else:\n",
        "        scores_cite.append(0)"
      ],
      "metadata": {
        "id": "b-W1CSs-OkFp"
      },
      "id": "b-W1CSs-OkFp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions\n",
        "scores_cite = np.array(scores_cite)\n",
        "auc_cite = roc_auc_score(yte, scores_cite)\n",
        "pred_cite = (scores_cite > np.percentile(scores_cite, 99)).astype(int)\n",
        "prec, rec, f1_cite, _ = precision_recall_fscore_support(yte, pred_cite, average='binary')\n",
        "\n",
        "print(\"CITE (GMM + per-cluster IF) — AUC:\", auc_cite, \"F1:\", f1_cite)"
      ],
      "metadata": {
        "id": "jr96BHFYq0Ol"
      },
      "id": "jr96BHFYq0Ol",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supervised XGBoost Stream (SUPERVISED STREAM)"
      ],
      "metadata": {
        "id": "fzVEZHCrr2rr"
      },
      "id": "fzVEZHCrr2rr"
    },
    {
      "cell_type": "code",
      "source": [
        "dtrain = xgb.DMatrix(Xtr, label=ytr)\n",
        "dtest = xgb.DMatrix(Xte)\n",
        "\n",
        "params = {\n",
        "    'objective':'binary:logistic',\n",
        "    'eval_metric':'auc',\n",
        "    'seed':42,\n",
        "    'eta':0.05,\n",
        "    'max_depth':6\n",
        "}\n",
        "\n",
        "bst = xgb.train(params, dtrain, num_boost_round=200)\n",
        "\n",
        "p = bst.predict(dtest)\n",
        "auc_xgb = roc_auc_score(yte, p)\n",
        "print(\"XGBoost stream — AUC:\", auc_xgb)\n"
      ],
      "metadata": {
        "id": "povosnihru7h"
      },
      "id": "povosnihru7h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics"
      ],
      "metadata": {
        "id": "c63AYJuisB88"
      },
      "id": "c63AYJuisB88"
    },
    {
      "cell_type": "code",
      "source": [
        "# SHAP Interpretability\n",
        "explainer = shap.TreeExplainer(bst)\n",
        "shap_values = explainer.shap_values(Xte)\n",
        "\n",
        "plt.figure(figsize=(9,5))\n",
        "shap.summary_plot(shap_values, Xte, show=False)\n",
        "plt.title(\"SHAP Feature Importance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eJFcd2mVr4RB"
      },
      "id": "eJFcd2mVr4RB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Built Generic Evaluation Table\n",
        "table = pd.DataFrame({\n",
        "    'Model': ['Baseline IF', 'CITE-GMM-IF', 'XGBoost'],\n",
        "    'AUC': [auc_base, auc_cite, auc_xgb],\n",
        "    'F1': [f1, f1_cite, np.nan]\n",
        "})\n",
        "table"
      ],
      "metadata": {
        "id": "fqgvqFB8r9jR"
      },
      "id": "fqgvqFB8r9jR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Per-Cluster\n",
        "results = []\n",
        "for k in range(6):\n",
        "    idx = (cluster_te == k)\n",
        "    if idx.sum() == 0: continue\n",
        "\n",
        "    true = yte[idx]\n",
        "    preds = pred_cite[idx]\n",
        "\n",
        "    tn = ((true==0)&(preds==0)).sum()\n",
        "    fp = ((true==0)&(preds==1)).sum()\n",
        "    fpr = fp / (fp+tn+1e-9)\n",
        "\n",
        "    results.append([k, fpr])\n",
        "\n",
        "cluster_fprs = pd.DataFrame(results, columns=['cluster','FPR'])\n",
        "\n",
        "sns.barplot(data=cluster_fprs, x='cluster', y='FPR')\n",
        "plt.title(\"Per-Cluster FPR (Fairness)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OcV5OWkosDst"
      },
      "id": "OcV5OWkosDst",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Processed Feature File to CSV\n",
        "features.to_csv(\"processed_features.csv\", index=False)"
      ],
      "metadata": {
        "id": "xuoDa8fPsOGV"
      },
      "id": "xuoDa8fPsOGV",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}