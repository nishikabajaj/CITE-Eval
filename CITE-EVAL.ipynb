{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Syntethic Dataset Generator - CERT backup"
      ],
      "metadata": {
        "id": "LwZchGaLQJAu"
      },
      "id": "LwZchGaLQJAu"
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_synthetic(n_users=50, days=30):\n",
        "    rows = []\n",
        "    for u in range(n_users):\n",
        "        base_shift = np.random.choice([0,3,6,9])  # cultural proxy\n",
        "        for d in range(days):\n",
        "            login_count=np.random.poisson(5)\n",
        "            malicious = 1 if (np.random.rand()<0.02 and login_count>7) else 0\n",
        "            rows.append([u,d,login_count,base_shift,malicious])\n",
        "    df = pd.DataFrame(rows, columns=['user','day','login_count','shift','label'])\n",
        "    return df\n",
        "\n",
        "df = generate_synthetic()\n",
        "df.head()"
      ],
      "metadata": {
        "id": "YBV7Zm2eQIuX"
      },
      "id": "YBV7Zm2eQIuX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing & feature engineering"
      ],
      "metadata": {
        "id": "1WaEGiVXLKIE"
      },
      "id": "1WaEGiVXLKIE"
    },
    {
      "cell_type": "code",
      "source": [
        "# src/preprocess.py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib, os\n",
        "\n",
        "SEED = 42"
      ],
      "metadata": {
        "id": "rY8KMknnLQja"
      },
      "id": "rY8KMknnLQja",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_logs(path):\n",
        "  # Adapt to CERT version csv schema\n",
        "  return pd.read_csv(path, parse_dates=['timestamp'])"
      ],
      "metadata": {
        "id": "TNlthNHYMZ74"
      },
      "id": "TNlthNHYMZ74",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_ofVBud_MdOp"
      },
      "id": "_ofVBud_MdOp",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_cultural_proxies(agg):\n",
        "    # simple proxies - TODO: REFINE (time-zone via IP/host map, communication language via email metadata if available, but avoid PII)\n",
        "    # time zone proxy (simulate from login hours)\n",
        "    agg['local_time_offset'] = (agg['avg_hour'] - 12) / 24.0\n",
        "    # communication density proxy (placeholder)\n",
        "    agg['comm_density'] = np.log1p(agg['login_count'])\n",
        "    # team tenure / group inferred: randomly assign group for demo\n",
        "    np.random.seed(SEED)\n",
        "    agg['team_id'] = np.random.randint(0,6, size=len(agg))\n",
        "    return agg"
      ],
      "metadata": {
        "id": "6IjUQFp8MhIZ"
      },
      "id": "6IjUQFp8MhIZ",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_feature_matrix(agg, out_path='data/processed/features.pkl'):\n",
        "    feat_cols = ['login_count','unique_hosts','files_accessed','avg_hour',\n",
        "                 'local_time_offset','comm_density','team_id']\n",
        "    X = agg[feat_cols].copy()\n",
        "    # encode team_id as one-hot or leave as numeric for GMM\n",
        "    X = pd.get_dummies(X, columns=['team_id'], drop_first=True)\n",
        "    scaler = StandardScaler()\n",
        "    Xs = scaler.fit_transform(X)\n",
        "    joblib.dump((Xs, scaler, X.columns.tolist()), out_path)\n",
        "    print('Saved features to', out_path)\n",
        "    return Xs"
      ],
      "metadata": {
        "id": "i2fC0Lr7MkO5"
      },
      "id": "i2fC0Lr7MkO5",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "    df = load_logs('data/raw/logs.csv')\n",
        "    agg = window_features(df)\n",
        "    agg = add_cultural_proxies(agg)\n",
        "    Xs = build_feature_matrix(agg)"
      ],
      "metadata": {
        "id": "4H2PwNNsMl8w",
        "outputId": "f93b3c90-79cc-4008-d069-92cef4ff42e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "id": "4H2PwNNsMl8w",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3871878787.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# df = load_logs('data/raw/logs.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0magg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindow_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0magg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_cultural_proxies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mXs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_feature_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating Baseline Models"
      ],
      "metadata": {
        "id": "boACrOASN5Hm"
      },
      "id": "boACrOASN5Hm"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "u1AnXS5dN-aX"
      },
      "id": "u1AnXS5dN-aX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, scaler, cols = joblib.load('data/processed/features.pkl')\n",
        "# Suppose you have binary labels y (1=malicious event, 0=benign)\n",
        "# If CERT dataset, load labels accordingly. For now synthetic:\n",
        "y = np.zeros(X.shape[0], dtype=int)\n",
        "# split\n",
        "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Global IsolationForest\n",
        "clf = IsolationForest(n_estimators=200, contamination=0.01, random_state=42)\n",
        "clf.fit(Xtr)\n",
        "scores = -clf.decision_function(Xte)   # higher = anomalous\n",
        "# metrics\n",
        "auc = roc_auc_score(yte, scores)\n",
        "prec,rec,f1,_ = precision_recall_fscore_support(yte, (scores>np.percentile(scores,99)).astype(int), average='binary', zero_division=0)\n",
        "print('IF AUC',auc,'F1',f1)"
      ],
      "metadata": {
        "id": "KyFkaVYYOHS4"
      },
      "id": "KyFkaVYYOHS4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing CITE Pipeline"
      ],
      "metadata": {
        "id": "GV7K1JgLONP3"
      },
      "id": "GV7K1JgLONP3"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "gmm = GaussianMixture(n_components=6, covariance_type='full', random_state=42)\n",
        "gmm.fit(Xtr)\n",
        "cluster_probs = gmm.predict_proba(Xte)   # π_u,t\n",
        "cluster_assign = cluster_probs.argmax(axis=1)"
      ],
      "metadata": {
        "id": "zF7heuCVOS2s"
      },
      "id": "zF7heuCVOS2s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mns8z8S1OVVG"
      },
      "id": "mns8z8S1OVVG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Per-cluster IsolationForest + thresholds**\n",
        "\n",
        "For each cluster k:\n",
        "\n",
        "Train an IsolationForest on training data where cluster_assign==k (or high membership).\n",
        "\n",
        "Compute anomaly scores for validation set and set a threshold τ_k to achieve target cluster alert rate or target FPR on val set."
      ],
      "metadata": {
        "id": "5AZ65YR8OXap"
      },
      "id": "5AZ65YR8OXap"
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "cluster_if = {}\n",
        "thresholds = {}\n",
        "for k in range(gmm.n_components):\n",
        "    idx = (gmm.predict_proba(Xtr).argmax(axis=1) == k)\n",
        "    if idx.sum() < 50:\n",
        "        continue\n",
        "    clf_k = IsolationForest(n_estimators=200, random_state=42).fit(Xtr[idx])\n",
        "    cluster_if[k] = clf_k\n",
        "    scores_k = -clf_k.decision_function(Xtr[idx])\n",
        "    # threshold as 99th percentile\n",
        "    thresholds[k] = np.percentile(scores_k, 99)"
      ],
      "metadata": {
        "id": "b-W1CSs-OkFp"
      },
      "id": "b-W1CSs-OkFp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RL / Simple bandit for threshold adaptation (practical approach)**\n",
        "\n",
        "Full RL is heavy. For an evaluation, use a contextual bandit / epsilon-greedy to adjust τ_k to control Δ-FPR. This is simpler, reproducible, and measurable.\n",
        "\n",
        "Algorithm (per evaluation window):\n",
        "\n",
        "For each cluster k, maintain current τ_k.\n",
        "\n",
        "Sample small adjustments ±δ.\n",
        "\n",
        "Use validation set or delayed labels to compute ΔFPR_k.\n",
        "\n",
        "If ΔFPR_k > target, increase τ_k (make less sensitive), else decrease.\n",
        "\n",
        "(You can implement a Q-table over discretized thresholds or keep it greedy.)"
      ],
      "metadata": {
        "id": "7lWKdbpYOrl-"
      },
      "id": "7lWKdbpYOrl-"
    },
    {
      "cell_type": "code",
      "source": [
        "# for each epoch:\n",
        "for epoch in range(E):\n",
        "    for k in clusters:\n",
        "        candidate = thresholds[k] + np.random.choice([-delta,0,delta])\n",
        "        # evaluate on held-out val window -> compute new FPR_k\n",
        "        if new_delta_fpr < old_delta_fpr: thresholds[k]=candidate"
      ],
      "metadata": {
        "id": "UwdnT_wnOxx_"
      },
      "id": "UwdnT_wnOxx_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Supervised Stream**\n",
        "\n",
        "Construct supervised labels y. Train XGBoost on labeled events with class weighting. Include cluster probs as features and per-cluster anomaly scores."
      ],
      "metadata": {
        "id": "E4LljsiGO4Ce"
      },
      "id": "E4LljsiGO4Ce"
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "dtrain = xgb.DMatrix(Xtr, label=ytr)\n",
        "params = {'objective':'binary:logistic','eval_metric':'auc','scale_pos_weight': max(1, (len(ytr)-ytr.sum())/max(1,ytr.sum())), 'seed':42}\n",
        "bst = xgb.train(params, dtrain, num_boost_round=200)\n",
        "dtest = xgb.DMatrix(Xte)\n",
        "p = bst.predict(dtest)\n"
      ],
      "metadata": {
        "id": "XNVBWK5eO9FV"
      },
      "id": "XNVBWK5eO9FV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = shap.TreeExplainer(bst)\n",
        "shap_values = explainer.shap_values(Xte)\n",
        "shap.summary_plot(shap_values, features=... )"
      ],
      "metadata": {
        "id": "4gCRi9evPBok"
      },
      "id": "4gCRi9evPBok",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation & metrics (exact formulas)\n",
        "\n",
        "Compute and report:\n",
        "\n",
        "AUC-ROC: roc_auc_score(y_true, score)\n",
        "\n",
        "Macro F1: precision_recall_fscore_support(..., average='macro')\n",
        "\n",
        "Per-cluster FPR: for each k, FPR_k = FP_k / (FP_k + TN_k)\n",
        "\n",
        "Δ-FPR: max_k |FPR_k - FPR_global|\n",
        "\n",
        "Cultural Robustness Index (CRI) (your paper proposes it): define concretely, e.g.\n",
        "\n",
        "Statistical test: use paired bootstrap to compare F1 between baseline and CITE; report p-value.\n",
        "\n",
        "Plotting:\n",
        "\n",
        "Boxplots of anomaly scores by cluster\n",
        "\n",
        "Line chart: thresholds over epochs\n",
        "\n",
        "Bar chart: FPR_k for baseline vs CITE\n",
        "\n",
        "SHAP summary\n",
        "\n",
        "Use matplotlib/seaborn."
      ],
      "metadata": {
        "id": "euFWHtbfPFWQ"
      },
      "id": "euFWHtbfPFWQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ablation studies (must-have)\n",
        "\n",
        "Run these and report results in a table:\n",
        "\n",
        "Baseline global IF\n",
        "\n",
        "GMM + per-cluster IF (no RL adaptation)\n",
        "\n",
        "GMM + per-cluster IF + threshold adaptation (bandit/RL)\n",
        "\n",
        "Full CITE: add supervised XGBoost stream (combine logic described in paper)\n",
        "\n",
        "CITE without cultural proxies (ablate cultural features)\n",
        "\n",
        "CITE with noisy/different proxy sets (robustness)\n",
        "\n",
        "Report Δ-FPR and macro-F1 for each."
      ],
      "metadata": {
        "id": "moZ8VnR_PMAb"
      },
      "id": "moZ8VnR_PMAb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evasion & robustness experiment (recommended for impact)\n",
        "\n",
        "Implement an attack where an insider “cluster-hops”: they modify behavior to look like cluster j to evade detection (simulate by swapping feature distributions).\n",
        "\n",
        "Measure detection rate for baseline vs CITE.\n",
        "\n",
        "Show CITE either resists or specify the limits."
      ],
      "metadata": {
        "id": "gcPRE8XjPPao"
      },
      "id": "gcPRE8XjPPao"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}